#include "NativeLearner.h"
#include <rudra/MLPparams.h>
#include <rudra/Network.h>
#include <rudra/io/GPFSSampleClient.h>
#include <rudra/io/UnifiedBinarySampleReader.h>
#include <rudra/io/UnifiedBinarySampleSeqReader.h>
#include <rudra/math/Matrix.h>
#include <rudra/param/PSUtils.h>
#include <rudra/util/Logger.h>
#include <rudra/util/RudraRand.h>
#include <iostream>

// refactor later
namespace xrudra {

    long NativeLearner::getNumEpochs() {
        return rudra::MLPparams::_numEpochs;
    }

    long NativeLearner::getNumTrainingSamples() {
        return rudra::MLPparams::_numTrainSamples;
    }

    long NativeLearner::getMBSize() {
        return rudra::MLPparams::_batchSize;
    }

  // begin static methods setting fields in MLPparams
  void NativeLearner::setLoggingLevel(int level) {
    rudra::Logger::setLoggingLevel(level);
  }
  void NativeLearner::setJobDir(std::string jobDir) {
    rudra::MLPparams::setJobID(jobDir);
  }

  void NativeLearner::setAdaDeltaParams(float rho, float epsilon,
                                        float drho, float depsilon) {
   if (rho != drho) rudra::MLPparams::_adaDeltaRho = rho;
    if (epsilon != depsilon) rudra::MLPparams::_adaDeltaEpsilon = epsilon;
  }
  void NativeLearner::setMeanFile(std::string fn){
    rudra::MLPparams::_meanFile = fn;
    //    NativeLearner::setMoM(0.0);
    //    NativeLearner::setLRMult(2);
  }

  void NativeLearner::setSeed(long id, int seed, int defaultSeed) {
    if (seed != defaultSeed) {
      rudra::MLPparams::_randSeed=seed;
      srand(seed);
    } else {
      struct timeval start;
      gettimeofday(&start, NULL);
      unsigned int sd = (unsigned int) ((float) start.tv_usec / (float)( (id+1)*(id+1) ));
      srand(sd);
    }
  }

  void NativeLearner::setMoM(float mom) {
    rudra::MLPparams::_mom = mom;    
  }

  void NativeLearner::setLRMult(float mult) {
    rudra::MLPparams::_lrMult = mult;
  }

  void NativeLearner::setWD() {
    rudra::MLPparams::setWD();
  }

  void NativeLearner::initFromCFGFile(std::string confName) {
    rudra::MLPparams::initMLPparams(confName);
  }
  // end statics

  NativeLearner::NativeLearner(long id) :
    nn(NULL), 
    pid(id),
    trainSC(NULL)
  {}

    void NativeLearner::cleanup() {
        if (nn != NULL) delete nn;
        if (trainSC != NULL) delete trainSC;
        if (psu != NULL) delete psu;
    }

// write the output per checkptinterval or at the end of job 
  void NativeLearner::checkpointIfNeeded(int whichEpoch) {
    int chkptInterval = rudra::MLPparams::_chkptInterval;
    int epochNum = rudra::MLPparams::_numEpochs;
    std::string jobID = rudra::MLPparams::_jobID;

    if ((chkptInterval != 0 && whichEpoch % chkptInterval == 0) || whichEpoch == epochNum) {
      std::string outputFileName = "";
      if (whichEpoch == epochNum) {
        outputFileName = jobID + ".final" + ".h5";
      } else {
        std::stringstream ss;
        ss << whichEpoch;
        outputFileName = jobID + ".epoch." + ss.str() + ".h5";
      }
      std::cout << "[Tester] Writing weights to " << outputFileName << std::endl;
      nn->writeParamsH5(outputFileName);
      if (psu->solverType==rudra::ADAGRAD)
        psu->chkptAdaGrad("ada"+outputFileName);
    }
  }
/**
 * added on Aug 12, 2015, to support compatible weights update as in c++ code
 */
void NativeLearner::initPSU(std::string _solverType){
// step 5, initilized PSU
    rudra::SolverType solverType;
    if (_solverType.compare("sgd") == 0){
	solverType = rudra::SGD;
    } else if (_solverType.compare("adagrad") == 0){
	solverType = rudra::ADAGRAD;
    } else if (_solverType.compare("adadelta") == 0) {
      solverType = rudra::ADADELTA;
    }  else {
	std::cerr
          <<"incorrect solver type specified, going to use SGD solver type instead."
          <<std::endl;
    }
    assert(nn != NULL);
    psu = new rudra::PSUtils(nn, solverType);
}

/**
 * init as learner agent
 */
void NativeLearner::initAsLearner(std::string weightsFile, std::string solverType) {
    initNetwork(weightsFile);

    // initialize GPFS Sample client for reading training and test data
    char agentName[21];
    sprintf(agentName, "LearnerAgent %6ld", pid);
    trainSC = new rudra::GPFSSampleClient(std::string(agentName), false,
        new rudra::UnifiedBinarySampleReader(rudra::MLPparams::_trainData, 
                                             rudra::MLPparams::_trainLabels, 
                                             rudra::RudraRand(pid, pid)));

    initPSU(solverType);
}

void NativeLearner::initAsTester(long placeID, std::string solverType) {
    initNetwork("");
    initPSU(solverType);
}

/**
 * initialize the network
 */
  int NativeLearner::initNetwork(std::string weightsFile){
    nn = rudra::Network::readFromConfigFile(rudra::MLPparams::MLPCfg["layerCfgFile"]);
    if (nn != NULL) {
      if (rudra::MLPparams::_mom != RUDRA_DEFAULT_INT) 
        nn->setMomentum(rudra::MLPparams::_mom);

      // ensure mul-rate takes effect at the very beginning
      updateLearningRate(0);

      if (!weightsFile.empty()) nn -> readParamsH5(weightsFile);
      return 0;
    } else { 
      std::cerr<<"failed to initialize the neural network."<<std::endl;
      return 1;
    }
  }

int NativeLearner::getNetworkSize() {
  return nn->networkSize;
}

float NativeLearner::trainMiniBatch() {
    rudra::Matrix<float> minibatchX(rudra::MLPparams::_batchSize, rudra::MLPparams::_numInputDim);
    rudra::Matrix<float> minibatchY(rudra::MLPparams::_batchSize, trainSC->getSizePerLabel());
    trainSC->getLabelledSamples(minibatchX.buf, minibatchY.buf);
    return nn->trainNetworkNoUpdate(minibatchX, minibatchY);
}

void NativeLearner::getGradients(float *gradients) {
  nn->serializeUpdates(gradients);
}

void NativeLearner::accumulateGradients(float *gradients) {
  nn->accumulateUpdates(gradients);
}

/**
 * update learning rate
 */
void NativeLearner::updateLearningRate(long curEpochNum){
// compatible with update learning rate in the new scheme
    std::cout << "NativeLearner::updateLearningRate pid " << pid
        << " epoch " << curEpochNum << " learning rate " 
        << rudra::MLPparams::_lrMult*rudra::MLPparams::LearningRateMultiplier::_lr[curEpochNum]
        << std::endl;
    nn->mulLearningRate(rudra::MLPparams::_lrMult
                        *rudra::MLPparams::LearningRateMultiplier::_lr[curEpochNum]); 
}

void NativeLearner::serializeWeights(float *weights){
  nn->serialize(weights);
}

void NativeLearner::deserializeWeights(float *weights){
  nn->deserialize(weights);
}


/**
 * assuming this call is always 


#define UU_NOP_FLAG  1 // just plain sum update
#define UU_SMB_FLAG 2 // finish a "super mb" (i.e., super mini-batch size)
#define UU_EPOCH_FLAG 4 // finish the entire epoch (now the time to send a test to test server) 
 */

/**
 *
 */
void NativeLearner::acceptGradients(float *grad, size_t numMB){
    psu->applyUpdateAfterSum(grad, numMB);
}

float NativeLearner::testOneEpochSC(float *weights, size_t numTesters) {
    char agentName[21];
    sprintf(agentName, "TestClient %6ld", pid);
    size_t batchSize = std::min(rudra::MLPparams::_numTestSamples,
                                rudra::MLPparams::_batchSize);
    size_t numMB = std::max((size_t) 1, rudra::MLPparams::_numTestSamples / batchSize);
    size_t mbPerLearner = std::max((size_t) 1, numMB / numTesters);
    size_t startMB = pid * mbPerLearner;
    size_t numSamplePerLearner = rudra::MLPparams::_numTestSamples / numTesters + 1;
    float totalTestErr = 0.0f;
    if (startMB < numMB) {
        size_t cursor = startMB * rudra::MLPparams::_batchSize;
        rudra::GPFSSampleClient testSC(std::string(agentName), true, 
            new rudra::UnifiedBinarySampleSeqReader(rudra::MLPparams::_testData, 
                                                    rudra::MLPparams::_testLabels, 
                                                    numSamplePerLearner, cursor));
        nn->deserialize(weights);
        rudra::Matrix<float> minibatchX(rudra::MLPparams::_batchSize, rudra::MLPparams::_numInputDim);
        rudra::Matrix<float> minibatchY(rudra::MLPparams::_batchSize, testSC.getSizePerLabel());
        for (size_t i = 0; i < mbPerLearner; i++) {
          testSC.getLabelledSamples(minibatchX.buf, minibatchY.buf);
          totalTestErr += nn->testNetworkMinibatch(minibatchX, minibatchY);
        }
    }
    float testError = totalTestErr / mbPerLearner;
    // cosmetic changes, to return a percentage, instead of a fraction
    return testError * 100;
  }

}


