#include "NativeLearner.h"
#include <rudra/io/UnifiedBinarySampleReader.h>
#include <rudra/io/UnifiedBinarySampleSeqReader.h>
#include "rudra/util/Logger.h"
#include <rudra/util/RudraRand.h>
#include <iostream>

// refactor later
namespace xrudra {
  NativeLearner::NativeLearner() :
    nn(NULL), 
    trainMBErr(-1.0f),
    trainSC(NULL),
    NUM_LEARNER(0),
    NUM_MB_PER_EPOCH(0),
    solverType(rudra::SGD),
    testErr(-1.0f)
  {}

void NativeLearner::setMeanFile(std::string fn){
    rudra::MLPparams::_meanFile = fn;
    rudra::MLPparams::_mom = 0.0;
    rudra::MLPparams::_lrMult =2;
}

  void NativeLearner::initNativeLand(long id, const char* confName, 
                                     long numLearner){
    // step 1 place id, seed random number generator
    pid = id;
    struct timeval start;
    gettimeofday(&start, NULL);
    unsigned int seed = (unsigned int) ((float) start.tv_usec / (float)( (id+1)*(id+1) ));
    srand(seed);
    // step 2 init the configuration
    //    rudra::Logger::setLoggingLevel(INFO);
    cfgFile = std::string(confName);
    rudra::MLPparams::initMLPparams(cfgFile);
 
    // step 3 init numLearner
    NUM_LEARNER = numLearner;

    // step 4 initialize NUM_MB_PER_EPOCH e.g., ParamServer.cpp:16-23, at most do L+1/L amount of work
    int tmpMBPerEpoch = rudra::MLPparams::_numTrainSamples / rudra::MLPparams::_batchSize;
    int q = tmpMBPerEpoch / numLearner;
    int r = tmpMBPerEpoch % numLearner;
    if(r == 0){
	NUM_MB_PER_EPOCH = tmpMBPerEpoch;
    }else{
	NUM_MB_PER_EPOCH = (q+1) * numLearner;
    }

    
    

}
/**
 * added on Aug 12, 2015, to support compatible weights update as in c++ code
 */
void NativeLearner::initPSU(std::string _solverType){
// step 5, initilized PSU
    if(_solverType.compare("sgd") == 0){
	solverType = rudra::SGD;
    }else if(_solverType.compare("adagrad") == 0){
	solverType = rudra::ADAGRAD;
    }else{
	std::cerr<<"incorrect solver type specified, going to use SGD solver type instead."<<std::endl;
    }
    assert(nn != NULL);
    psu = new rudra::PSUtils(nn, solverType);
}

/**
 * init as learner agent
 */
void NativeLearner::initAsLA(bool isReconciler){
    initNetwork(isReconciler);     // init network
   // added on May 1, 2015 to init minibatch
    initXY();
    initTrainSC();
}

void NativeLearner::initXY(){
    minibatchX = rudra::Matrix<float>(rudra::MLPparams::_numInputDim, rudra::MLPparams::_batchSize);
    minibatchY = rudra::Matrix<float>(rudra::MLPparams::_numClasses, rudra::MLPparams::_batchSize);
}


void NativeLearner::initTrainSC(){
    char agentName[21];
    sprintf(agentName, "LearnerAgent %6d", pid);
    trainSC = new rudra::GPFSSampleClient(std::string(agentName), false,
        new rudra::UnifiedBinarySampleReader(rudra::MLPparams::_trainData, rudra::MLPparams::_trainLabels, rudra::RudraRand(pid, pid)));
    
}

/**
 * initialize the network
 */
int NativeLearner::initNetwork(bool isReconciler){
  nn = rudra::Network::readFromConfigFile(rudra::MLPparams::MLPCfg["layerCfgFile"]);
  nn->setMomentum(0.0);
    nn->mulLearningRate(rudra::MLPparams::_lrMult*rudra::MLPparams::LearningRateMultiplier::_lr[0]); // to make sure the mul takes effect to begin with     
  if(nn != NULL){
    return 0;
  }else{
    std::cerr<<"failed to initialize the neural network."<<std::endl;
    return 1;
  }
}

int NativeLearner::getNetworkSize() {
  return nn->networkSize;
}

void NativeLearner::loadMiniBatch(){
    //sc->getLabelledSamples(rudra::MLPparams::_batchSize, minibatchX, minibatchY);
    trainSC->getLabelledSamples(minibatchX, minibatchY);
}

float NativeLearner::trainMiniBatch(){
    trainMBErr = nn->trainNetworkNoUpdate(minibatchX, minibatchY); // who will gc minibatchX and minibatchY ?
    return trainMBErr;
    //std::cout<<"here here trainMBErr "<<trainMBErr<<std::endl;

    // TODO: return the Rail of the updates
}

void NativeLearner::getGradients(float *updates) {
	nn->serializeUpdates(updates);
}

void NativeLearner::accumulateGradients(float *updates) {
	nn->accumulateUpdates(updates);
}

/**
 * update learning rate
 */
void NativeLearner::updateLearningRate(long curEpochNum){
    //nn->setLearningRate(rudra::MLPparams::_alphaDecay);
    nn->mulLearningRate(rudra::MLPparams::_lrMult*rudra::MLPparams::LearningRateMultiplier::_lr[curEpochNum]); // compatible with update learning rate in the new scheme
}


//////////////////// methods on the side of param server ///////////////

void NativeLearner::serializeWeights(float *weights){
    nn->serialize(weights);
}

void NativeLearner::deserializeWeights(float *weights){
    nn->deserialize(weights);
}


/**
 * assuming this call is always 
 */

#define UU_NOP_FLAG  1 // just plain sum update
#define UU_SMB_FLAG 2 // finish a "super mb" (i.e., super mini-batch size)
#define UU_EPOCH_FLAG 4 // finish the entire epoch (now the time to send a test to test server) 


/**
 *
 */
void NativeLearner::acceptGradients(float *grad, size_t numMB){
    psu->applyUpdateAfterSum(grad, numMB);
}

////////////////// SS Section //////////////////////////


float NativeLearner::testOneEpoch(float *weights){
  rudra::Network* nn = rudra::Network::readFromConfigFile(rudra::MLPparams::MLPCfg["layerCfgFile"]);

  //    rudra::Network *nn = new rudra::Network(rudra::MLPparams::MLPCfg["layerCfgFile"],
  //				       rudra::MLPparams::MLPCfg["logDir"] + "log");
      nn->deserialize(weights);
      std::cout<<"[in native land] before testing network, network size: "<<nn->networkSize<<std::endl;
      testErr = nn->testNetwork(testData, testLabels);
      std::cout<<"[in naitve land] after testing network, testErr: "<<testErr<<std::endl;
      delete nn;
      return testErr;
}

float NativeLearner::testOneEpoch(){
  testErr = nn->testNetwork(testData, testLabels);
  return testErr;
}

  //rudra::GPFSSampleClient *NativeLearner::testSC = NULL;
void NativeLearner::initTestSC(){
	char agentName[21];
	sprintf(agentName, "TestClient %6d", pid);
	rudra::UnifiedBinarySampleSeqReader *binReader =
				new rudra::UnifiedBinarySampleSeqReader(rudra::MLPparams::_testData, rudra::MLPparams::_testLabels, rudra::MLPparams::_numTestSamples);
	testSC = new rudra::GPFSSampleClient(std::string(agentName), true, binReader);
}

// added on Aug 13, 2015, to support parallel file reading for testing
void NativeLearner::initTestSC(long placeID, size_t numLearner){
	char agentName[21];
	sprintf(agentName, "TestClient %6d", pid);
	size_t numSamplePerLearner = rudra::MLPparams::_numTestSamples / numLearner + 1;
	size_t cursor = placeID * numSamplePerLearner;
	rudra::UnifiedBinarySampleSeqReader *binReader =
	    new rudra::UnifiedBinarySampleSeqReader(rudra::MLPparams::_testData, rudra::MLPparams::_testLabels, numSamplePerLearner, cursor);
	NativeLearner::testSC = new rudra::GPFSSampleClient(std::string(agentName), true, binReader);
}

float NativeLearner::testOneEpochSC(float *weights){
    assert(testSC != NULL );
	size_t batchSize = std::min(rudra::MLPparams::_numTestSamples,
			rudra::MLPparams::_batchSize);
	size_t minibatchSize = rudra::MLPparams::_numTestSamples / batchSize;
	size_t numMB = std::max((size_t) 1, minibatchSize);
	rudra::Matrix<float> testX, testY;
	testErr = 0;
	rudra::Network *nn = rudra::Network::readFromConfigFile(rudra::MLPparams::MLPCfg["layerCfgFile"]);  
	      nn->deserialize(weights);
	for (size_t i = 0; i < numMB; i++) {
	  testSC->getLabelledSamples(testX, testY);
		float _testErr = nn->testNetworkMinibatch(testX, testY);
		testErr = (testErr * i + _testErr) / (i + 1); // running average of testErr
	}
	return (testErr * 100); // cosmetic changes, to return a percentage number, instead of a fraction number.
}

    long NativeLearner::getTestNum(){
	return rudra::MLPparams::_numTestSamples;
    }
}

////////////////// end of SS Section //////////////////
